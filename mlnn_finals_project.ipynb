{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4702ac00",
   "metadata": {},
   "source": [
    "# TO BE DELETED\n",
    "## Overview\n",
    "- Follow the universal workflow of DLWP 4.5 (1st edition) for a dataset of your choice.\n",
    "- You can use the tensorflow datasets, MNIST, Reuters, IMDB and Boston Housing Price, or an external dataset. Work exclusively in a Jupyter notebook.\n",
    "- You can only use DLWP Part 1 layers (Chapters 1 - 4) i.e. restrict your models to tensorflow sequential Dense and Dropout layers.\n",
    "- Your Jupyter notebook should read as a report, not just a sequence of code cells. Structure your report with markdown headings, subheadings, tables etc.\n",
    "- You can use as much DLWP code and code from the video notebooks as you wish but you must reference all code that is not original: credit will be given for model assembly using third-party code, and extra credit may be awarded for original code.\n",
    "- Export your Jupyter notebook to html and submit. Do not submit your notebook or any data files. Submit only the html export of your notebook.\n",
    "\n",
    "### NOT FOR ME\n",
    "(For Colab users: Colab does not have an html facility. Either:\n",
    "- Download the colab notebook and load into Jupyter; you will then be able to export as html OR follow the instructions in this blog i.e. download from colab and then reload into colab's session storage.\n",
    "- Then run the script: %%shelljupyter nbconvert --to html /Your notebook path/file.ipynb and download the html version)\n",
    "\n",
    "## REVIEW CRITERIA\n",
    "Credit will be awarded for:\n",
    "- report structure and quality as a document\n",
    "- adherence to the deep learning workflow\n",
    "- a systematic investigation\n",
    "- interpretation of results\n",
    "\n",
    "Additional credit may be awarded for:\n",
    "- modular programming\n",
    "- external dataset (but not an online tutorial dataset)\n",
    "- extensive experimentation\n",
    "- understanding and technique that exceeds the module syllabus\n",
    "\n",
    "## DLWP 4.5 CHAPTER SUMMARY\n",
    "- Define the problem at hand and the data on which you’ll train. Collect this data, or annotate it with labels if need be\n",
    "- Choose how you’ll measure success on your problem. Which metrics will you monitor on your validation data?\n",
    "- Determine your evaluation protocol: hold-out validation? K-fold validation? Which portion of the data should you use for validation?\n",
    "- Develop a first model that does better than a basic baseline: a model with statistical power\n",
    "- Develop a model that overfits\n",
    "- Regularize your model and tune its hyperparameters, based on performance on the validation data. A lot of machine-learning research tends to focus only on this step—but keep the big picture in mind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772e5c8a",
   "metadata": {},
   "source": [
    "# The universal workflow of machine learning\n",
    "This project centers around the universal workflow of **DLWP 4.5** on the **CIFAR100** small images classification datatset in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe046f7",
   "metadata": {},
   "source": [
    "## 0 Defining the problem\n",
    "### Datatypes and goals\n",
    "- What will your input data be?\n",
    "- What are you trying to predict?\n",
    "\n",
    "```You can only learn to predict something if you have available training data: for example, you can only learn to classify the sentiment of movie reviews if you have both movie reviews and sentiment annotations available. As such, data availability is usually the limiting factor at this stage (unless you have the means to pay people to collect data for you).```\n",
    "\n",
    "### Problem type\n",
    "```Is it binary classification? Multiclass classification? Scalar regression? Vector regression? Multiclass, multilabel classification? Something else, like clustering, generation, or reinforcement learning? Identifying the problem type will guide your choice of model architecture, loss function, and so on.```\n",
    "\n",
    "### Hypothesis\n",
    "- You hypothesize that your outputs can be predicted given your inputs\n",
    "- You hypothesize that your available data is sufficiently informative to learn the relationship between inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fce5d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARATION FOR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c06d3",
   "metadata": {},
   "source": [
    "## 1 Assembling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1e0b72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a34f7cb",
   "metadata": {},
   "source": [
    "## 2 Choosing a measure of success\n",
    "```To control something, you need to be able to observe it. To achieve success, you must define what you mean by success—accuracy? Precision and recall? Customer-retention rate? Your metric for success will guide the choice of a loss function: what your model will optimize. It should directly align with your higher-level goals, such as the success of your business. For balanced-classification problems, where every class is equally likely, accuracy and area under the receiver operating characteristic curve (ROC AUC) are common metrics. For class-imbalanced problems, you can use precision and recall. For ranking problems or multilabel classification, you can use mean average precision. And it isn’t uncommon to have to define your own custom metric by which to measure success. To get a sense of the diversity of machine-learning success metrics and how they relate to different problem domains, it’s helpful to browse the data science competitions on Kaggle (https://kaggle.com); they showcase a wide range of problems and evaluation metrics.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d125188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91454892",
   "metadata": {},
   "source": [
    "## 3 Deciding on an evaluation protocol\n",
    "```Once you know what you’re aiming for, you must establish how you’ll measure your current progress. We’ve previously reviewed three common evaluation protocols:```\n",
    "\n",
    "- Maintaining a hold-out validation set: The way to go when you have plenty of data\n",
    "- Doing K-fold cross-validation: The right choice when you have too few samples for hold-out validation to be reliable\n",
    "- Doing iterated K-fold validation: For performing highly accurate model evaluation when little data is available\n",
    "\n",
    "```Just pick one of these. In most cases, the first will work well enough.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1758ee42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85209a9d",
   "metadata": {},
   "source": [
    "## 4 Preparing your data\n",
    "```Once you know what you’re training on, what you’re optimizing for, and how to evaluate your approach, you’re almost ready to begin training models. But first, you should format your data in a way that can be fed into a machine-learning model — here, we’ll assume a deep neural network:```\n",
    "\n",
    "- As you saw previously, your data should be formatted as tensors\n",
    "- The values taken by these tensors should usually be scaled to small values: for example, in the [-1, 1] range or [0, 1] range\n",
    "- If different features take values in different ranges (heterogeneous data), then the data should be normalized\n",
    "- You may want to do some feature engineering, especially for small-data problems\n",
    "\n",
    "```Once your tensors of input data and target data are ready, you can begin to train models```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecede37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f955a567",
   "metadata": {},
   "source": [
    "## 5 Developing a model that does better than a baseline\n",
    "```Your goal at this stage is to achieve statistical power: that is, to develop a small model that is capable of beating a dumb baseline. In the MNIST digit-classification example, anything that achieves an accuracy greater than 0.1 can be said to have statistical power; in the IMDB example, it’s anything with an accuracy greater than 0.5.```\n",
    "\n",
    "```Note that it’s not always possible to achieve statistical power. If you can’t beat a random baseline after trying multiple reasonable architectures, it may be that the answer to the question you’re asking isn’t present in the input data. Remember that you make two hypotheses:```\n",
    "\n",
    "- You hypothesize that your outputs can be predicted given your inputs\n",
    "- You hypothesize that the available data is sufficiently informative to learn the relationship between inputs and outputs\n",
    "\n",
    "```It may well be that these hypotheses are false, in which case you must go back to the drawing board.```\n",
    "```Assuming that things go well, you need to make three key choices to build your first working model:```\n",
    "\n",
    "- Last-layer activation: This establishes useful constraints on the network’s output. For instance, the IMDB classification example used sigmoid in the last layer; the regression example didn’t use any last-layer activation; and so on\n",
    "- Loss function: This should match the type of problem you’re trying to solve. For instance, the IMDB example used binary_crossentropy, the regression example used mse, and so on\n",
    "- Optimization configuration: What optimizer will you use? What will its learning rate be? In most cases, it’s safe to go with rmsprop and its default learning rate\n",
    "\n",
    "```Regarding the choice of a loss function, note that it isn’t always possible to directly optimize for the metric that measures success on a problem. Sometimes there is no easy way to turn a metric into a loss function; loss functions, after all, need to be computable given only a mini-batch of data (ideally, a loss function should be computable for as little as a single data point) and must be differentiable (otherwise, you can’t use backpropagation to train your network). For instance, the widely used classification metric ROC AUC can’t be directly optimized. Hence, in classification tasks, it’s common to optimize for a proxy metric of ROC AUC, such as crossentropy. In general, you can hope that the lower the crossentropy gets, the higher the ROC AUC will be. Table 4.1 can help you choose a last-layer activation and a loss function for a few common problem types.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189be080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59c44631",
   "metadata": {},
   "source": [
    "## 6 Scaling up: developing a model that overfits\n",
    "```Once you’ve obtained a model that has statistical power, the question becomes, is your model sufficiently powerful? Does it have enough layers and parameters to properly model the problem at hand? For instance, a network with a single hidden layer with two units would have statistical power on MNIST but wouldn’t be sufficient to solve the problem well. Remember that the universal tension in machine learning is between optimization and generalization; the ideal model is one that stands right at the border between underfitting and overfitting; between undercapacity and overcapacity. To figure out where this border lies, first you must cross it.```\n",
    "\n",
    "```To figure out how big a model you’ll need, you must develop a model that overfits. This is fairly easy:```\n",
    "1. Add layers\n",
    "2. Make the layers bigger\n",
    "3. Train for more epochs\n",
    "\n",
    "```Always monitor the training loss and validation loss, as well as the training and validation values for any metrics you care about. When you see that the model’s performance on the validation data begins to degrade, you’ve achieved overfitting.```\n",
    "\n",
    "```The next stage is to start regularizing and tuning the model, to get as close as possible to the ideal model that neither underfits nor overfits.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7ff7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58db56dc",
   "metadata": {},
   "source": [
    "## 7 Regularizing your model and tuning your hyperparameters\n",
    "```This step will take the most time: you’ll repeatedly modify your model, train it, evaluate on your validation data (not the test data, at this point), modify it again, and repeat, until the model is as good as it can get. These are some things you should try:```\n",
    "\n",
    "- Add dropout\n",
    "- Try different architectures: add or remove layers\n",
    "- Add L1 and/or L2 regularization\n",
    "- Try different hyperparameters (such as the number of units per layer or the learning rate of the optimizer) to find the optimal configuration\n",
    "- Optionally, iterate on feature engineering: add new features, or remove features that don’t seem to be informative\n",
    "\n",
    "```Be mindful of the following: every time you use feedback from your validation process to tune your model, you leak information about the validation process into the model. Repeated just a few times, this is innocuous; but done systematically over many iterations, it will eventually cause your model to overfit to the validation process (even though no model is directly trained on any of the validation data). This makes the evaluation process less reliable.```\n",
    "\n",
    "```Once you’ve developed a satisfactory model configuration, you can train your final production model on all the available data (training and validation) and evaluate it one last time on the test set. If it turns out that performance on the test set is significantly worse than the performance measured on the validation data, this may mean either that your validation procedure wasn’t reliable after all, or that you began overfitting to the validation data while tuning the parameters of the model. In this case, you may want to switch to a more reliable evaluation protocol (such as iterated K-fold validation).```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ecdd1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
